{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical Illustrations\n",
    "\n",
    "This notebook reproduces the numerical studies in Section 5 of this [paper](https://arxiv.org/pdf/2301.00260.pdf) and Chapter 2.6 of this [dissertation](https://digital.lib.washington.edu/researchworks/bitstream/handle/1773/49764/Liu_washington_0250E_25117.pdf?sequence=1&isAllowed=y). If you find it useful, please cite\n",
    "\n",
    "```\n",
    "@inproceedings{liu2022likelihood,\n",
    "  title={Likelihood Score under Generalized Self-Concordance},\n",
    "  author={Liu, Lang and Harchaoui, Zaid},\n",
    "  booktitle={NeurIPS 2022 Workshop on Score-Based Methods},\n",
    "  year={2022}\n",
    "}\n",
    "```\n",
    "and\n",
    "```\n",
    "@phdthesis{liu2022statistical,\n",
    "  title={Statistical Divergences for Learning and Inference: Limit Laws and Non-Asymptotic Bounds},\n",
    "  author={Liu, Lang},\n",
    "  year={2022},\n",
    "  school={University of Washington}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.patches import Ellipse\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "COLORS = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "\n",
    "mpl.rcParams['lines.linewidth'] = 3\n",
    "mpl.rcParams['xtick.labelsize'] = 12\n",
    "mpl.rcParams['ytick.labelsize'] = 12\n",
    "mpl.rcParams['axes.labelsize'] = 18\n",
    "mpl.rcParams['legend.fontsize'] = 18\n",
    "mpl.rcParams['axes.titlesize'] = 18\n",
    "mpl.rcParams['lines.markersize'] = 7.5\n",
    "mpl.rcParams['text.usetex'] = False\n",
    "mpl.rcParams[\"mathtext.fontset\"] = 'cm'\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "\n",
    "LINESTYLE = ['-', '--', '-.', (0, (1, 1)), '-', '--']\n",
    "MARKER = ['8', 's', '', '', '^', '8']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shape of Confidence Set\n",
    "\n",
    "In this section we illustrate the shape of the confidence set we considered on a logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate data\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def generate_data(n, cov, theta0):\n",
    "    X = np.random.multivariate_normal(np.zeros(2), cov, size=n)\n",
    "    prob = sigmoid(-X @ theta0)\n",
    "    Y = np.random.uniform(size=n)\n",
    "    Y = (Y >= prob)*1\n",
    "    Y = np.array((Y - 0.5)*2, dtype=int)\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "# construct confidence set\n",
    "def estimate(X, Y):\n",
    "    model = LogisticRegression(random_state=0, fit_intercept=False).fit(X, Y)\n",
    "    return model.coef_[0]\n",
    "\n",
    "\n",
    "def conf_set(delta, X, theta_hat):\n",
    "    prob = sigmoid(X @ theta_hat)\n",
    "    emp_hess = X.T @ np.diag(prob * (1 - prob)) @ X / len(X)\n",
    "    chol = np.linalg.cholesky(emp_hess)\n",
    "    angle = np.linspace(0, 2*np.pi, 100)\n",
    "    b = 0.15*np.sqrt(np.log(10/delta))\n",
    "    w = np.vstack((b * np.cos(angle), b * np.sin(angle)))\n",
    "    w = np.linalg.solve(chol, w).T\n",
    "    return theta_hat + w\n",
    "\n",
    "\n",
    "# compute losses\n",
    "def single_loss(w0, w1, x, y):\n",
    "    return np.log(1 + np.exp(-(x[0]*w0 + x[1]*w1)*y))\n",
    "\n",
    "\n",
    "def loss(w0, w1, X, Y):\n",
    "    losses = [single_loss(w0, w1, X[i], Y[i]) for i in range(n)]\n",
    "    return np.mean(losses, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(51722)\n",
    "n = 1000\n",
    "theta0 = np.array([-1.0, 2.0])\n",
    "covs = [np.array([[2, 0], [0, 1]]), np.array([[2, 1], [1, 1]]), np.array([[2, -1], [-1, 1]])]\n",
    "levels = [[0.45, 0.5, 0.6, 0.75, 1.0], [0.55, 0.65, 0.85, 1.1, 1.4], [0.37, 0.4, 0.45, 0.55, 0.7]]\n",
    "ellipses = [Ellipse([0, 0], 0.9, 1.95, angle=7, alpha=0.3, color=COLORS[1], label=r'$\\delta = 0.95$'),\n",
    "            Ellipse([0, 0], 0.68, 2.33, angle=9, alpha=0.3, color=COLORS[1], label=r'$\\delta = 0.95$'),\n",
    "            Ellipse([0, 0], 1.5, 2.55, angle=-6, alpha=0.3, color=COLORS[1], label=r'$\\delta = 0.95$')]\n",
    "\n",
    "# for loss contour\n",
    "w0 = np.linspace(-3, 1, 100)\n",
    "w1 = np.linspace(0, 4, 100)\n",
    "W0, W1 = np.meshgrid(w0, w1)\n",
    "\n",
    "nfig = len(covs)\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, sharey=True)\n",
    "fig.set_figheight(4)\n",
    "fig.set_figwidth(12)\n",
    "for k, cov in enumerate(covs):\n",
    "    X, Y = generate_data(n, cov, theta0)\n",
    "    theta_hat = estimate(X, Y)\n",
    "    losses = loss(W0, W1, X, Y)\n",
    "    conf = conf_set(0.95, X, theta_hat)\n",
    "    ellipse = ellipses[k]\n",
    "    ellipse.set(center=theta_hat)\n",
    "    \n",
    "    axes[k].tick_params(\n",
    "        axis='both',\n",
    "        which='both',\n",
    "        bottom=False,\n",
    "        top=False,\n",
    "        left=False,\n",
    "        labelbottom=False\n",
    "    )\n",
    "    \n",
    "    axes[k].contour(W0, W1, losses, levels=levels[k], colors='gray')\n",
    "    axes[k].plot([0], [0], label='Empirical risk contour', color='gray')\n",
    "    axes[k].scatter(theta0[0], theta0[1], s=150, color=COLORS[3], marker='*', label=r'$\\theta_0$')\n",
    "    axes[k].scatter(theta_hat[0], theta_hat[1], s=150, color=COLORS[0], marker='*', label=r'$\\theta_n$')\n",
    "    axes[k].add_artist(ellipse)\n",
    "    \n",
    "handles, labels = axes[0].get_legend_handles_labels()\n",
    "fig.tight_layout()\n",
    "lgd = fig.legend(\n",
    "    handles, labels, loc='lower center',\n",
    "    bbox_to_anchor=(0.5, -0.14), ncol=4\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimation of the Effective Dimension\n",
    "\n",
    "In this section we estimate the effective dimension from data and investigate its estimation error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Least squares\n",
    "def generate_least_square_data(n, cov, theta0):\n",
    "    X = np.random.multivariate_normal(np.zeros(len(cov)), cov, size=n)\n",
    "    Y = X @ theta0 + np.random.standard_normal(n)\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "def ls_empirical_effective_dim(X, Y, theta_hat):\n",
    "    res = X @ theta_hat - Y\n",
    "    grad = res[:, np.newaxis] * X\n",
    "    G_n = grad.T @ grad / len(Y)\n",
    "    H_n = X.T @ X / len(Y)\n",
    "    return np.trace(np.linalg.solve(H_n, G_n))\n",
    "\n",
    "\n",
    "def ls_compute_abs_err(n, cov, theta0):\n",
    "    X, Y = generate_least_square_data(n, cov, theta0)\n",
    "    theta_hat = np.linalg.solve(X.T @ X, X.T @ Y)\n",
    "    d_n = ls_empirical_effective_dim(X, Y, theta_hat)\n",
    "    return np.abs(d_n / len(theta0) - 1)\n",
    "\n",
    "\n",
    "# Logistic regression\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def logistic_estimate(X, Y):\n",
    "    model = LogisticRegression(random_state=0, fit_intercept=False).fit(X, Y)\n",
    "    return model.coef_[0]\n",
    "\n",
    "\n",
    "def generate_logistic_data(n, cov, theta0, cond_prob):\n",
    "    X = np.random.multivariate_normal(np.zeros(len(cov)), cov, size=n)\n",
    "    prob = cond_prob(X, theta0)\n",
    "    Y = np.random.uniform(size=n)\n",
    "    Y = (Y <= prob)*1\n",
    "    Y = np.array((Y - 0.5)*2, dtype=int)\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "def logistic_empirical_effective_dim(X, Y, theta_hat):\n",
    "    prob_hat = sigmoid(Y * (X @ theta_hat))\n",
    "    grad = ((prob_hat - 1) * Y)[:, np.newaxis] * X\n",
    "    G_n = grad.T @ grad / len(Y)\n",
    "    hess_sqrt = np.sqrt(prob_hat * (1 - prob_hat))[:, np.newaxis] * X\n",
    "    H_n = hess_sqrt.T @ hess_sqrt / len(Y)\n",
    "    return np.trace(np.linalg.solve(H_n, G_n))\n",
    "\n",
    "\n",
    "def logistic_effective_dim(X, cond_prob, theta0, theta):\n",
    "    prob = cond_prob(X, theta0)\n",
    "    prob_hat = sigmoid(X @ theta)\n",
    "    grad = np.sqrt((1 - prob_hat)**2 * prob + prob_hat**2 * (1 - prob))[:, np.newaxis] * X\n",
    "    G_star = grad.T @ grad / len(Y)\n",
    "    hess_sqrt = np.sqrt(prob_hat * (1 - prob_hat))[:, np.newaxis] * X\n",
    "    H_star = hess_sqrt.T @ hess_sqrt / len(Y)\n",
    "    return np.trace(np.linalg.solve(H_star, G_star))\n",
    "\n",
    "\n",
    "def cond_prob(X, theta0):\n",
    "    return sigmoid(X @ theta0)\n",
    "\n",
    "\n",
    "def logistic_compute_abs_err(n, cov, theta0):\n",
    "    X, Y = generate_logistic_data(n, cov, theta0, cond_prob)\n",
    "    theta_hat = logistic_estimate(X, Y)\n",
    "    d_n = logistic_empirical_effective_dim(X, Y, theta_hat)\n",
    "    return np.abs(d_n / len(theta0) - 1)\n",
    "\n",
    "\n",
    "# Plot\n",
    "def plot_abs_err(drange, nranges, abs_errs, repeat=10, fname=None, save=False):\n",
    "    fig_prefix = '.'\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, sharey=True)\n",
    "    fig.set_figheight(4.5)\n",
    "    fig.set_figwidth(8)\n",
    "    axes[0].set_ylabel(r'$\\mathbb{E}| d_n / d_\\star - 1 |$')\n",
    "    axes[0].set_title(f'Least squares')\n",
    "    axes[1].set_title(f'Logistic regression')\n",
    "\n",
    "    for k, abs_err in enumerate(abs_errs):\n",
    "        for i, d in enumerate(drange):\n",
    "            err = np.array([e[0] for e in abs_err[i]])\n",
    "            se = np.array([e[1]/np.sqrt(repeat) for e in abs_err[i]])\n",
    "            axes[k].plot(\n",
    "                nranges[k], err, label=f'd = {d}', color=COLORS[i],\n",
    "                linestyle=LINESTYLE[i], marker=MARKER[i])\n",
    "            axes[k].fill_between(nranges[k], err-se, err+se, color=COLORS[i], alpha=0.3)\n",
    "\n",
    "        axes[k].set_xlabel('Sample Size')\n",
    "\n",
    "    handles, labels = axes[0].get_legend_handles_labels()\n",
    "    fig.tight_layout(rect=[0, 0.08, 1, 1])  # L, B, R, T\n",
    "    lgd = fig.legend(\n",
    "        handles, labels, loc='lower center',\n",
    "        bbox_to_anchor=(0.5, -0.02), ncol=4)\n",
    "\n",
    "    if save:\n",
    "        fig.savefig(\n",
    "            f'{fig_prefix}/{fname}.pdf',\n",
    "            bbox_extra_artists=[lgd], bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_nrange = np.linspace(2000, 10000, 12, dtype=int)\n",
    "logistic_nrange = np.linspace(2000, 10000, 12, dtype=int)\n",
    "drange = [5, 10, 15, 20]\n",
    "repeat = 30\n",
    "\n",
    "ls_abs_err = []\n",
    "logistic_abs_err = []\n",
    "for d in drange:\n",
    "    ls_err = []\n",
    "    logistic_err = []\n",
    "    cov = np.identity(d)\n",
    "    theta0 = np.ones(d)\n",
    "    for i, n in enumerate(ls_nrange):\n",
    "        ls_tmp = []\n",
    "        logistic_tmp = []\n",
    "        for _ in range(repeat):\n",
    "            ls_tmp.append(ls_compute_abs_err(n, cov, theta0))\n",
    "            logistic_tmp.append(logistic_compute_abs_err(logistic_nrange[i], cov, theta0))\n",
    "        ls_err.append((np.mean(ls_tmp), np.std(ls_tmp)))\n",
    "        logistic_err.append((np.mean(logistic_tmp), np.std(logistic_tmp)))\n",
    "    ls_abs_err.append(ls_err)\n",
    "    logistic_abs_err.append(logistic_err)\n",
    "    \n",
    "plot_abs_err(drange, [ls_nrange, logistic_nrange], [ls_abs_err, logistic_abs_err], repeat=repeat,\n",
    "             fname='est-effective-dim', save=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('py3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a634188f845e843d9457f8c6868d42775da245e74de5a0359d1c1cea92534153"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
